#!/usr/bin/env python

import argparse
import xml.parsers.expat
import socket
import json
import time
import sys
import zlib
import httplib
import logging
import logging.handlers

class gmeta_processor:
    #
    # Processes XML stats from ganglia gmetad and injects them into
    # elasticsearch via the bulk API. Designed to run as a deamon
    # process collecting stats at a regular interval and pushing to
    # ES.
    #
    def __init__ (
            self,
            gmeta_hosts,
            elastic_hosts,
            interval = 15,
            batch    = 10000,
            debug    = False,
            basename = 'ganglia',
            utctime  = True,
    ):
        self.debug = debug
        self.gmeta_hosts = gmeta_hosts
        self.elastic_hosts = elastic_hosts
        self.interval = interval
        self.batch = batch
        self.cur_host = None
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.time_reported = 0
        self.readbytes = 8192
        self.basename = basename
        #
        # The localtime function does not give proper tz info
        #
        self.utctime = utctime
        if utctime:
            self.tz = '+0000'
        else:
            self.tz = time.strftime('%z')
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0
        #
        # Start elasticsearch keepalive connection if not debug mode
        #
        if not debug:
            self.elastic_connect()

    def s_el (self, name, attrs):
        #
        # XML start element processor. For grid and cluster types
        # store the name. For host type weget the reported type and
        # store host name and number processed. Metric is where the
        # real magic happens of created index entries
        #
        if name == 'GRID':
            self.cur_grid = attrs['NAME']
        if name == 'CLUSTER':
            self.cur_cluster = attrs['NAME']
        if name == 'HOST':
            self.cur_host = attrs['NAME']
            self.time_reported = int(attrs['REPORTED'])
            self.n_parsed_hosts += 1
        if name == 'METRIC' and self.cur_host:
            self.n_parsed_metrics += 1
            tn = int(attrs['TN'])
            time_metric = self.check_time - tn
            #
            # Metrics reported less than interval time ago should be
            # pushed to the system. Maybe we also need to check that
            # host last updated time is within last interval
            # (otherwise we may have already done this one)
            #
            if tn > self.interval:
                return
            if self.utctime:
                metric_reftime = time.gmtime(time_metric)
            else:
                metric_reftime = time.localtime(time_metric)

            time_metric = time.strftime(
                "%Y-%m-%dT%H:%M:%S", metric_reftime
            )
            time_metric += self.tz
            units = attrs['UNITS']
            if units.strip() == "":
                units = "-"
            #
            # The index itself, which gets converted to json
            #
            out = {
                'name': attrs['NAME'],
                'units': units,
                '@timestamp': time_metric,
                'host': self.cur_host,
                'cluster': self.cur_cluster,
                'grid': self.cur_grid,
            }
            val = attrs['VAL']
            if attrs['TYPE'] == 'float' or attrs['TYPE'] == 'double':
                out['val'] = float(val)
            elif attrs['TYPE'] != 'string':
                out['val'] = int(val)
            else:
                out['stringval'] = val
            #
            # Override the index if needed. Dont send each time to
            # conserve on bytes
            #
            indexname = time.strftime(
                self.basename+'-%Y.%m.%d', metric_reftime
            )
            if indexname == self.indexname:
                self.buffer.append('{"index":{}}')
            else:
                self.buffer.append(
                    '{"index":{"_index":"%s"}}' %(indexname)
                )
            self.buffer.append(json.dumps(out))

    def e_el (self, name):
        #
        # XML end element parser. All we need to do here is close out
        # host entry so no processing any cruft
        #
        global host
        if name == 'HOST':
            self.cur_host = None

    def elastic_connect (self):
        #
        # Try connecting to one of any specified elasticsearch nodes
        # in order and return 1 if successful or 0 if failed. The
        # connection is stored and used in the elastic_index function.
        #
        for e in self.elastic_hosts:
            addr,port = e.split(':')
            self.elasticconn = httplib.HTTPConnection(addr,int(port))
            try:
                self.elasticconn.connect()
                return 1
            except:
                logging.warning('failed to connect es host %s:%s' %(addr,port))

        logging.error('failed to connect to any es host')
        return 0

    def elastic_index (self):
        #
        # Index whatever is in the buffer at present. Attempt a
        # reconnect to elasticsearch and inspect the output for errors
        # if required and configured
        #
        logging.debug('Submitting batch of size %d to index %s' %(
            len(self.buffer)/2, self.indexname,
        ))

        if self.debug:
            logging.debug('indexing disabled')
            return
        #
        # Skip if we have nothing to index
        #
        if len(self.buffer) == 0:
            return

        bulk = '\n'.join(self.buffer)
        req = '/%s/gmon/_bulk' %(self.indexname)
        hdr = { "Content-Type":"text/json" }
        #
        # Try sending the request to elasticsearch on current
        # connection. This may fail if elastic goes down or otherwise
        # closes the connection. Retry connection on all supplied
        # servers and reissue request if possible, otherwise we have
        # no choice but to give up
        #
        try:
            self.elasticconn.request('POST', req, bulk, hdr)
        except socket.error:
            if self.elastic_connect():
                self.elasticconn.request('POST', req, bulk, hdr)
            else:
                return

        #
        # Attempt to get the response from server, if this fails we
        # are probably in a bad state so clean up connection and, and
        # skip this one (we quite possibly sent already)
        #
        try:
            res = self.elasticconn.getresponse()
        except:
            logging.error('Got exception in es http response:',sys.exc_info())
            self.elasticconn.close()
            self.elastic_connect()
            return
        #
        # We have to read the response here for reusing the
        # connection, for non OK statuses as well
        #
        try:
            rdata = json.loads(res.read())
        except:
            return

        if res.status == 200:
            if rdata['errors']:
                logging.warning('errors detected in es response')
                #
                # possibly inspect errors in indexing. We should
                # probably only do this if some debug level logging is
                # requested, otherwise just continue
                #
                c = 0
                for i in rdata['items']:
                    if i['create']['status'] != 201:
                        logging.debug('index error source: %s, message: %s' %(
                            self.buffer[c*2+1],i
                        ))
                    c+=1
        else:
            logging.error('Got bad return from es: %d %s' %(
                res.status, res.reason
            ))

    def parse_metrics (self):
        #
        # Connect to Ganglia and get XML representation of
        # cluster. Feed into the XML parser and occasionally trigger
        # the elastic bulk indexing after batching (batch parameter)
        #
        p=xml.parsers.expat.ParserCreate()
        p.StartElementHandler = self.s_el
        p.EndElementHandler = self.e_el
        xmldata=''
        #
        # connect to ganglia. Try servers listed in turn for a server
        # which we can connect to
        #
        for gmeta in self.gmeta_hosts:
            s = socket.socket()
            addr,port = gmeta.split(':')
            try:
                s.connect((addr,int(port)))
                xmldata = s.recv(self.readbytes)
                break
            except:
                logging.error('Could not ganglia source %s:%s' %(addr,port))
                pass
        #
        # check if we never connected, then skip the rest of this run if so
        #
        if xmldata == '':
            logging.error('Failed to connect to any ganglia source')
            return

        # clear counters and defaults
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.check_time = time.time()
        # set the presumed index name (overidden if needed)
        self.indexname = time.strftime(self.basename+'-%Y.%m.%d')
        # check for zipped data, use zlib autdetection MAX_WBITS|32
        zipped = False
        if xmldata[0:5] != '<?xml':
            zipped = True
            decomp = zlib.decompressobj(zlib.MAX_WBITS|32)
        while xmldata != '':
            try:
                if zipped:
                    p.Parse(decomp.decompress(xmldata))
                else:
                    p.Parse(xmldata)
            except:
                logging.error('Error %d parsing XML: %s at %d' %(
                    p.ErrorCode,
                    xml.parsers.expat.ErrorString(p.ErrorCode),
                    p.ErrorByteIndex,
                ))

            if len(self.buffer) >= self.batch:
                self.elastic_index()
                self.buffer = []

            xmldata = s.recv(self.readbytes)

        s.close()
        self.elastic_index()
        self.buffer = []


    def run (self):
        while True:
            start_time = time.time()
            self.parse_metrics()
            logging.debug('parsed %d metrics from %d hosts' %(
                self.n_parsed_metrics, self.n_parsed_hosts,
            ))
            sleep_time = self.interval-(time.time()-start_time)
            if sleep_time < 0:
                sleep_time = 0
            logging.debug('sleeping %f seconds' %(sleep_time))
            time.sleep(sleep_time)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Index ganglia metrics to Elasticsearch')
    parser.add_argument(
        '-g','--ganglia',
        type=str,help='gmetad host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-e','--elastic',
        type=str,help='elastic host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-i','--interval',
        type=int,help='poll in seconds',
        metavar='I',default=15,
    )
    parser.add_argument(
        '-n','--noindex',
        help='Do not index, just show what would happen',
        action='store_true',
    )
    parser.add_argument(
        '-b','--basename',
        help='base name of index e.g. {basename}-YYYY.mm.dd',
        type=str,default='ganglia',
    )
    parser.add_argument(
        '--localtime',
        help='use localtime instead of UTC for index and events',
        action='store_true',
    )
    parser.add_argument(
        '-v','--verbose',
        help='increase verbosity, -vv for debug logs',
        action='count',default=0,
    )
    parser.add_argument(
        '-c','--console',
        help='log to console instead of syslog',
        action='store_true',
    )

    p = parser.parse_args()

    #
    # setup our logging based on any passed option
    #
    rootlog = logging.getLogger()
    syslog  = logging.handlers.SysLogHandler('/dev/log')
    syslog.setFormatter(logging.Formatter('gtoesd: %(message)s'))
    #
    # Set the verbosity, starting at default of warning, we should
    # subtract 10 to get to next level for each -v option
    #
    rootlog.setLevel(logging.WARNING-10*p.verbose)
    if p.console:
        rootlog.addHandler(logging.StreamHandler())
    else:
        rootlog.addHandler(syslog)
    logging.debug('initialized logging interface')

    if not p.ganglia:
        p.ganglia = ['localhost:8651']
    if not p.elastic:
        p.elastic = ['localhost:9200']

    g = gmeta_processor(
        p.ganglia,
        p.elastic,
        interval=p.interval,
        debug=p.noindex,
        basename=p.basename,
        utctime=not p.localtime,
    )
    g.run()
