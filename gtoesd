#!/usr/bin/env python

import argparse
import xml.parsers.expat
import socket
import json
import time
import sys

class gmeta_processor:
    #
    # Processes XML stats from ganglia gmetad and injects them into
    # elasticsearch via the bulk API. Designed to run as a deamon
    # process collecting stats at a regular interval and pushing to
    # ES.
    #
    def __init__ (self, gmeta_hosts, elastic_hosts,
                  interval=15, batch=10000, debug=False):
        self.debug = debug
        self.gmeta_hosts = gmeta_hosts
        self.elastic_hosts = elastic_hosts
        self.interval = interval
        self.batch = batch
        self.cur_host = None
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.time_reported = 0
        self.readbytes = 8192
        #
        # The localtime function does not give proper tz info
        #
        self.tz = time.strftime('%z')
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0

    def s_el (self, name, attrs):
        #
        # XML start element processor. For grid and cluster types
        # store the name. For host type weget the reported type and
        # store host name and number processed. Metric is where the
        # real magic happens of created index entries
        #
        if name == 'GRID':
            self.cur_grid = attrs['NAME']
        if name == 'CLUSTER':
            self.cur_cluster = attrs['NAME']
        if name == 'HOST':
            self.cur_host = attrs['NAME']
            self.time_reported = int(attrs['REPORTED'])
            self.n_parsed_hosts += 1
        if name == 'METRIC' and self.cur_host:
            self.n_parsed_metrics += 1
            tn = int(attrs['TN'])
            time_metric = self.time_reported - tn
            #
            # Metrics reported less than interval time ago should be
            # pushed to the system. Maybe we also need to check that
            # host last updated time is within last interval
            # (otherwise we may have already done this one)
            #
            if tn > self.interval:
                return
            metric_localtime = time.localtime(time_metric)
            time_metric = time.strftime(
                "%Y-%m-%dT%H:%M:%S", metric_localtime
            )
            time_metric += self.tz
            val = attrs['VAL']
            if attrs['TYPE'] == 'float' or attrs['TYPE'] == 'double':
                val = float(val)
            elif attrs['TYPE'] != 'string':
                val = int(val)
            units = attrs['UNITS']
            if units.strip() == "":
                units = "-"
            #
            # The index itself, which gets converted to json
            #
            out = {
                'name': attrs['NAME'],
                'val':  val,
                'units': units,
                '@timestamp': time_metric,
                'host': self.cur_host,
                'cluster': self.cur_cluster,
                'grid': self.cur_grid,
            }
            #
            # Override the index if needed. Dont send each time to
            # conserve on bytes
            #
            indexname = time.strftime(
                'ganglia-%Y.%m.%d', metric_localtime
            )
            if indexname == self.indexname:
                self.buffer.append('{"index":{}}')
            else:
                self.buffer.append(
                    '{"index":{"_index":"%s"}}' %(indexname)
                )
            self.buffer.append(json.dumps(out))

    def e_el (self, name):
        #
        # XML end element parser. All we need to do here is close out
        # host entry so no processing any cruft
        #
        global host
        if name == 'HOST':
            self.cur_host = None

    def elastic_index (self):
        if self.debug:
            print 'debug mode: would index %d to index %s' %(
                len(self.buffer)/2,self.indexname)
            f=open('indexout.json','w')
            f.write('\n'.join(self.buffer))
            f.close()
            return
        bulk = '\n'.join(self.buffer)
        s = socket.socket()
        addr,port = self.elastic_hosts[0].split(':')
        s.connect((addr,int(port)))
        hdr = 'POST /%s/gmon/_bulk HTTP/1.1\n' %(self.indexname) +\
              'Content-Type: text/json\n' +\
              'Content-Length: %d\n\n' %(len(bulk))
        s.send(hdr+bulk)
        r = s.recv(256)
        print 'got back from elasticsearch:'
        print r
        if r.split('\n')[0].split(' ')[1] == '200':
            print 'ok status'

    def parse_metrics (self):
        #
        # Connect to Ganglia and get XML representation of
        # cluster. Feed into the XML parser and occasionally trigger
        # the elastic bulk indexing after batching (batch parameter)
        #
        p=xml.parsers.expat.ParserCreate()
        p.StartElementHandler = self.s_el
        p.EndElementHandler = self.e_el
        xmldata=''
        #
        # connect to ganglia. Try servers listed in turn for a server
        # which we can connect to
        #
        for gmeta in self.gmeta_hosts:
            s = socket.socket()
            addr,port = gmeta.split(':')
            try:
                s.connect((addr,int(port)))
                xmldata = s.recv(self.readbytes)
                break
            except:
                # print log here when logging added
                pass
        # clear counters and defaults
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.check_time = time.time()
        # set the presumed index name (overidden if needed)
        self.indexname = time.strftime('ganglia-%Y.%m.%d')
        while xmldata != '':
            try:
                p.Parse(xmldata)
            except:
                print 'got error (%d: %s) parsing XML at %d' %(
                    p.ErrorCode,
                    xml.parsers.expat.ErrorString(p.ErrorCode),
                    p.ErrorByteIndex,
                )
                print 'error string -----',xmldata[p.ErrorByteIndex],'-----'
                print xmldata[p.ErrorByteIndex-10:p.ErrorByteIndex+10]
                sys.exit()

            if len(self.buffer) >= self.batch:
                print 'going to send batch of size %d to es' %(len(self.buffer))
                self.elastic_index()
                self.buffer = []

            xmldata = s.recv(self.readbytes)

        print 'going to send batch of size %d to es' %(len(self.buffer))
        self.elastic_index()
        self.buffer = []


    def run (self):
        while True:
            start_time = time.time()
            self.parse_metrics()
            print 'parsed %d metrics from %d hosts' %(self.n_parsed_metrics,self.n_parsed_hosts)
            sleep_time = self.interval-(time.time()-start_time)
            print 'sleeping %f seconds' %(sleep_time)
            time.sleep(sleep_time)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Index ganglia metrics to Elasticsearch')
    parser.add_argument(
        '-g','--ganglia',
        type=str,help='gmetad host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-e','--elastic',
        type=str,help='elastic host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-i','--interval',
        type=int,help='poll in seconds',
        metavar='I',default=15,
    )
    parser.add_argument(
        '-n','--noindex',
        help='Do not index, just show what would happen',
        action='store_true',
    )

    p = parser.parse_args()

    if not p.ganglia:
        p.ganglia = ['localhost:8651']
    if not p.elastic:
        p.elastic = ['localhost:9200']

    g = gmeta_processor(
        p.ganglia,
        p.elastic,
        interval=p.interval,
        debug=p.noindex
    )
    g.run()
